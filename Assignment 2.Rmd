---
title: "R Notebook"
output:
  pdf_document: default
  html_document: 
    keep_md: yes
editor_options:
  chunk_output_type: inline
---

```{r}
options(scipen=999)
set.seed(1234)
LR2 <- read.table(file="./LR2.csv", header = TRUE, sep = ",")
names(LR2)
attach(LR2)
```

# Assignment 2

## Exercise 1

$$\text{Pr}(Y=1 | X=x)=\Phi(\beta_0+\beta_1x)$$
$$\Phi(x)=\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}\exp^{-\frac{1}{2}t^2}dt$$
```{r eval=FALSE, include=FALSE}
phi_pdf <- function(x) {
  cons <- sqrt(2*pi)^-1
  
  # ab <- seq(-10,x,by = 0.001)
  cons*exp(-(x^2/2))
}
#PDF
phi_pdf(0)  #0.3989423
dnorm(0,0,1)#0.3989423
```


$$\Phi(z)=\text{P}(Z \leq z), Z\sim\mathcal{N}(0,1)$$
Thus $\Phi(\beta_0+\beta_1x)=\text{P}(Z\leq z)$

Write an `R` function that computes the maximum likelihood estimate, `\mathscr{L} \left( \beta_0, \beta_1 \right)`, along with bootstrapped errors.

```{r}
# objective function
probit_mle_b <- function(x,y,...) {
  
  opts <- list(...)
  
  # probit link
  # 
  probit <- function(b,x,y) {
    n <- length(y)
    ll <- 0
    for(i in 1:n) {
      z <- b[1]+b[2]*x[i]
      z <- pnorm(z, mean=0, sd=1, log.p = FALSE)
      ll <- ll + log(z)*(y[i]==1) + log(1-z)*(y[i]==0)
    }
    
    return(-ll)
  }
  
  # mle
  # 
  obj = optim(c(0,0), probit, x=x, y=y)
  
  coef1 <- obj$par[1]
  coef2 <- obj$par[2]
  
  return_list <- list(
    model = obj,
    fitted = pnorm(coef1+coef2*x,0,1),
    coefficients = c(coef1,coef2)
  )
  
  ## Bootstrap
  ## 
  
  B <- 100
  
  b_boot = matrix(rep(0,2*B),B,2)
  n <- length(y) #n=1000
  for (i in 1:B) {
    # indices for the i-th bootstrap subsample
    ind_ = sample(n,n,replace=TRUE)
    # input vector in the subsample
    xb = x[ind_]
    # output vector in the subsample
    yb = y[ind_]
    
    # compute the maximum likelihood estimates
    obj = optim(c(0,0), probit, x=xb, y=yb)
    
    b_boot[i,1] = obj$par[1]
    b_boot[i,2] = obj$par[2]
  }
  
  return_list$standard_errors <- c(sd(b_boot[,1]),sd(b_boot[,2]))
  return_list$boots <- b_boot

  if(!is.null(opts)) {
    opts <- unlist(opts)
    return_list$response = ifelse(pnorm(coef1+coef2*opts,0,1)>1/2,1,0)
  }
    
  return(
    return_list
  )
}
```

```{r}
# Apply the probit estimator to LR2
# 
est <- probit_mle_b(LR2$x,LR2$y,x)
est$coefficients
#-4.242606 -3.086030

sum(diag(prop.table(table(est$response,y))))
#0.957

train <- LR2[1:800,]
test <- LR2[801:1000,]
est.2 <- probit_mle_b(train$x,train$y,test$x)
est.2$coefficients
#-4.557228 -3.213275

sum(diag(prop.table(table(est.2$response,test$y))))
#0.94
```

```{r}
glm.est <- suppressWarnings(glm(y~x,family=binomial(link = "probit")))
plot(x,y, pch=20, col=scales::alpha("black",alpha = 0.3))
abline(h=1, lty=2)
abline(h=0, lty=2)
# y0 <- sort(predict(glm.est,list(x),type="response"), decreasing = TRUE)
y1 <- sort(est$fitted,TRUE)
# lines(sort(x),y0,lwd=3,col="dodgerblue")
points(sort(x),y1,pch=20,cex=0.7,col=scales::alpha("darkorange",0.5))
```



## Exercise 2

Consider the model. Let $X$ and $U$ be two independent uniformly distributed random variables and let $Y$ be given by the equation

$$Y=I\left(U\leq \frac{1}{1+\exp(-\beta_0-\beta_1X-\beta_2X^3-\beta_3\log(X))}\right)$$
where 

$$  $$

The indicator function constructs a test that compares a standard logistic regression function and sample uniform random variable. The logistic function takes an explanatory variable $x$ drawn from the uniform random variable $X$, and computes a response. The response is compared to an independent sample of a random variable from the uniform distribution, $U$. Where the logistic response is greater than or equal to the uniform random variable, the indicator variable classifies 

```{r}
link <- function(b,n) {
  X <- runif(n)
  U <- runif(n)
  
  logit_X <- function(b,x) {
    (1 + exp(-b[1]-b[2]*x-b[3]*x^3-b[4]*log(x)))^-1
  }
  
  Y <- ifelse(U<=multi_logit(b,X),1,0)
  return(list(X=X,Y=Y))
}
```

```{r}

b <- c(-4,2,5,4)
n <- 1000

r <- link(b,n)
boxplot(X~Y,r)
```

Constructe a box and whiskar plot of test error rates.

```{r}

r.pred <- function(model) {
  prob <- predict(model,type="response")
  
  if (class(model)%in%c("lda","qda")) {
    pred <- prob$class
    return(
      pred
    )
  } else {
    pred <- rep(0,length(prob))
    pred[prob>0.5]=1
    # print(table(pred,model$fitted.values))
    return(
      #vector of predictions
      pred
    )
  }
}

#train 
#
#test

#linear probability model
r.lm <- lm(Y~X,r) 
r.lm.pred <- r.pred(r.lm)
mean(r.lm.pred==r$Y)

#logistic regression
r.glm <- glm(Y~X,r,family = binomial) 
r.glm.pred <- r.pred(r.glm)
mean(r.glm.pred==r$Y)

#linear discriminant analysis
r.lda <- MASS::lda(Y~X,r)
r.lda.pred <- r.pred(r.lda)
mean(r.lda.pred==r$Y)

#quadratic discriminant analysis
r.qda <- MASS::qda(Y~X,r) 
r.qda.pred <- r.pred(r.qda)
mean(r.qda.pred==r$Y)

class::knn(train=r$X,test=r$X,cl=r$Y,k=1)
```
